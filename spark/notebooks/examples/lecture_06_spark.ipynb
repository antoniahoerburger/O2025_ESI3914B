{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c883cb-c7c9-44ed-9b05-db69b4344589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987b5a84-391c-4f1a-8e76-8d55691c07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3152f-8db4-4ca9-839f-b9a8ccbf8ed7",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb94054-74b0-4d90-94d6-67ab7a8a4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81159dce-2f78-48eb-83b7-07de0e2498c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/05 14:31:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"1st spark session\") \\\n",
    "    .master(\"spark://e7f08de3a628:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73db94-bf92-4707-852c-dad8e0e80477",
   "metadata": {},
   "source": [
    "# Extract Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077626ec-57ce-499e-8361-41f1e6dc375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e18c3-f144-4e89-a166-e5dd614906fb",
   "metadata": {},
   "source": [
    "# Create First RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95231fee-336e-494b-8485-0ce436a7cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [53, 545, 24334, 654, 32435]\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e761db7-98d8-45ac-8eac-5d06483ba0f9",
   "metadata": {},
   "source": [
    "# Compute squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8abd6ec3-7bbd-4357-927b-f7a050691353",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = rdd.map(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c1dce-0d9f-49b8-af71-ed67bc9bddde",
   "metadata": {},
   "source": [
    "# Collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9ab503-6e32-4ed7-b36d-887883f44605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2809, 297025, 592143556, 427716, 1052029225]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_python = result_rdd.collect()\n",
    "result_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8af50-baec-4d9d-8bfd-59f5e7636e31",
   "metadata": {},
   "source": [
    "# Stop the Spark's context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42a2265-7815-4b4b-b3d5-02122e22884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593fd678-7821-40c5-ab06-2e523642a5ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/core/context.py:813\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[1;32m    782\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 813\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[1;32m    815\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/core/context.py:660\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    650\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "sc.parallelize(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
